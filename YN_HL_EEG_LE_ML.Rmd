---
title: "YN_EEG_LE_ML"
output: html_document
---

### Click [here](#ML_Context) to jump to the machine learning portion. 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results='hide', warning=FALSE, message=FALSE}
library(plyr)
#library(papeR)
library(dplyr)
library(ggplot2)
library(knitr)
library(kableExtra)
library(tidyr)
library(foreign)
library(multcomp)
library(broom)
library(nlme)
library(tidyverse)
library(randomForest)
library(glmnet)
library(readxl)
library(data.table)
library(caret)
library(xgboost)
library(DiagrammeR)
library("Ckmeans.1d.dp") # for xgb.ggplot.importance

options(scipen = 999) # turn off scientific notation. 

# set to turn html tables on or off 
html_tables=T 

#Set type of artifact rejection threshold 
Threshold= 50

Behav_path = ""
ONH_EEG_path <- "ONH/"
OHL_EEG_path <- "OHL/"
YN_EEG_path <- "YN/"
BaseCon_filename = 'BaseConArray.csv'
Word_filename ="WordTrialArray_Ep_0_0.7.csv"
Phrase_filename = "PhraseTrialArray_Ep_-0.8_0.csv"
Late_filename ="LateTrialArray_Ep_0.4_1.1.csv"

plot_finish =   theme_classic()+
  theme(plot.title = element_text(color= 'black',face='bold',size=18))+
   theme(axis.title.x = element_text(color= 'black',face='bold',size=18),
        axis.text.x = element_text(color= 'black',face='bold',size=18))+
  theme(axis.title.y = element_text(color= 'black',face='bold',size=18),
        axis.text.y = element_text(color= 'black',face='bold',size=18))+
 theme(legend.text = element_text( size = 18, face = "bold"))+
# theme(legend.position="none")+
    theme(# adjust X-axis labels; also adjust their position using margin (acts like a bounding box)
          # using margin was needed because of the inwards placement of ticks
          # http://stackoverflow.com/questions/26367296/how-do-i-make-my-axis-ticks-face-inwards-in-ggplot2
          axis.text.x = element_text( margin = unit(c(t = 2.5, r = 0, b = 0, l = 0), "mm")),
          # adjust Y-axis labels
          axis.text.y = element_text( margin = unit(c(t = 0, r = 2.5, b = 0, l = 0), "mm")),
          # length of tick marks - negative sign places ticks inwards
          axis.ticks.length = unit(-1.4, "mm"),
          # width of tick marks in mm
          axis.ticks = element_line(size = .8))

  find.r2 = function(pred, test, plot = TRUE, text = TRUE, output = TRUE){
    R.RMSE =  round(R2(pred,test), digits = 3)
            # Find Test RMSE
    RMSE = round(sqrt(mean((pred -  test)^2)), digits = 3)
    if(text == TRUE){
   print(paste('Test R Squared = ', R.RMSE))
     print(paste('Test RMSE = ',RMSE))
     }
    R.RMSE = c(R.RMSE, RMSE)
    if (plot == TRUE){
    val.min = min(c(pred, test))
    val.max = max(c(pred, test))
    plot(test,pred, abline(coef = c(0,1)), ylim = c(val.min, val.max), xlim = c(val.min, val.max))
    }
    if(output == TRUE){
    return(R.RMSE)     
    }
  }
```

## Import Behavioral data 

```{r, echo=FALSE, warning=FALSE}
# Import all data:::
## load in Self-Reprot and PC data

PTA_Demo_Raw=as.data.frame(read_excel(paste0(Behav_path, "EEG_LE_CDA_Data.xlsx"), sheet = 'Enter Data Here'))
PTA_Demo_Raw = dplyr::filter(PTA_Demo_Raw, Group %in% c("OHL","ONH", "YN"))
PTA_Demo_Raw[PTA_Demo_Raw == ""] = NA # empty cells are NA
PTA_Demo_Raw = Filter(function(x)!all(is.na(x)),PTA_Demo_Raw) # remove columns that are NA
PTA_Demo_Raw = PTA_Demo_Raw[,-c(147:149)]
colnames(PTA_Demo_Raw)[colnames(PTA_Demo_Raw)=="R LE LISN"] = "LISN__RLE"
colnames(PTA_Demo_Raw)[colnames(PTA_Demo_Raw)=="I LE LISN"] = "LISN__ILE"

colnames(PTA_Demo_Raw)[colnames(PTA_Demo_Raw)=="Subj#"] = "Sub"
colnames(PTA_Demo_Raw)[colnames(PTA_Demo_Raw)=="MoCA Raw Score"] = "MoCA"
PTA_Demo_Raw$MoCA = as.integer(PTA_Demo_Raw$MoCA)
colnames(PTA_Demo_Raw)[colnames(PTA_Demo_Raw)=="Adj. MoCA"] = "Adj_MoCA"
PTA_Demo_Raw$Adj_MoCA = as.integer(PTA_Demo_Raw$Adj_MoCA)
PTA_Demo_Raw$Sub=gsub("00", "_0", PTA_Demo_Raw$Sub)
PTA_Demo_Raw$Sub=gsub("N0", "N_", PTA_Demo_Raw$Sub)
PTA_Demo_Raw$Sub=gsub("L0", "L_", PTA_Demo_Raw$Sub)
PTA_Demo_Raw$Sub=gsub("H0", "H_", PTA_Demo_Raw$Sub)
PTA_Demo_Raw = droplevels(PTA_Demo_Raw) # drop all unused factor levels, for all factors in df!
PTA_Demo_Raw$Sub=as.factor(PTA_Demo_Raw$Sub)
PTA_Demo = PTA_Demo_Raw[,1:37]
PTA_Demo$Age = as.numeric(PTA_Demo$Age)
PTA_Demo$TFI = as.numeric(PTA_Demo$TFI)
PTA_Demo$PTA = as.numeric(PTA_Demo$PTA)
PTA_Demo$HF_PTA = as.numeric(PTA_Demo$HF_PTA)
PTA_Demo$MoCA = as.numeric(PTA_Demo$MoCA)
PTA_Demo$Education = as.numeric(PTA_Demo$Education)


PC_SR = PTA_Demo_Raw[,c(1:5,39:146)]

# ## load in Self-Reprot and PC data

# convert to long format and split on 6th char in Variable names
# using this example: https://stackoverflow.com/questions/25272018/split-column-name-and-convert-data-from-wide-to-long-format-in-r
# and this for help: https://ademos.people.uic.edu/Chapter9.html

PC_SR=PC_SR %>% gather(var, SR, LISN__RLE:WIN_R2_0dB_ILE)%>%
  separate(var, c("Run", "Condition"), sep = 6)


# remove leading and trailing "_"  
# help http://www.endmemo.com/program/R/gsub.php
PC_SR$Condition=gsub("^_", "", PC_SR$Condition)
PC_SR$Run=gsub("__$","", PC_SR$Run)
PC_SR$Run=gsub("_$","", PC_SR$Run)



# fix naming for sep "_" below
PC_SR$Condition=gsub("RLE", "LISN_RLE", PC_SR$Condition)
PC_SR$Condition=gsub("ILE", "LISN_ILE", PC_SR$Condition)
PC_SR$Condition=gsub("_LISN_", "_", PC_SR$Condition)


# split Condition name from Score type, ic splits on "_" 
PC_SR=PC_SR %>% separate(Condition,c("Condition", "Score_type"),
                         sep = "_") 

#tiddy vers way, change to tibble
PC_SR_tib = as_tibble(PC_SR) 
# arrange by subj., works! 
PC_SR_tib=PC_SR_tib%>% arrange(Sub)
# make wide format for self reports
PC_SR_tib=PC_SR_tib%>% spread(Score_type, SR)
# reorder columns
PC_SR_tib=PC_SR_tib[c(1:5,7,8, 6, 9:11)]
# rename columns
colnames(PC_SR_tib)[colnames(PC_SR_tib)=="Score"] = "WIN_Score"


# back to data.frame
PC_SR=data.frame(PC_SR_tib)
PC_SR$Run=gsub("LISN", "Baseline", PC_SR$Run)
PC_SR$Run=as.factor(PC_SR$Run)
PC_SR$Condition=as.factor(PC_SR$Condition)
PC_SR$WIN_Score=as.integer(PC_SR$WIN_Score)
PC_SR$LISN.Correct=as.integer(PC_SR$LISN.Correct)
PC_SR$ILE=as.integer(PC_SR$ILE)
PC_SR$RLE=as.integer(PC_SR$RLE)
rm(PC_SR_Raw)
rm(PC_SR_tib)
summary(PTA_Demo)
summary(PC_SR)
```


```{r, echo=FALSE,results='asis'}
## Load EEG data, append each group, make one list of all subjects with group ID:
Groups = c('YN', 'ONH', 'OHL')
Epochs = c('Phrase', 'Word', 'Late')
Phrase_list = list()
Word_list = list()
Late_list = list()

for (i in Groups) {
  path = paste(i, "_EEG_path",sep = "")
  Phrase_list[[i]] = fread(paste0(i,'/', Phrase_filename))
  Phrase_list[[i]]$Group = i #assign Group name
  Phrase_list[[i]] = na.omit(Phrase_list[[i]], cols='T2') # remove rows with NA 
}
Phrase= rbindlist(Phrase_list) # row bind the lists

for (i in Groups) {
  path = paste(i, "_EEG_path",sep = "")
  Word_list[[i]] = fread(paste0(i,'/', Word_filename))
  Word_list[[i]]$Group = i #assign Group name
  Word_list[[i]] = na.omit(Word_list[[i]], cols='T2') # remove rows with NA 
}
Word= rbindlist(Word_list) # row bind the lists

for (i in Groups) {
  path = paste(i, "_EEG_path",sep = "")
  Late_list[[i]] = fread(paste0(i,'/', Late_filename))
  Late_list[[i]]$Group = i #assign Group name
  Late_list[[i]] = na.omit(Late_list[[i]], cols='T2') # remove rows with NA 
}
Late= rbindlist(Late_list) # row bind the lists
# Epoch list of all EEG data
Epoch_List_Raw = list('Phrase'= Phrase, 'Word' = Word, 'Late' = Late)
# rm unused data:
rm(Phrase, Phrase_list, Word, Word_list, Late, Late_list)

Epoch_List =list()
 # lapply(Epoch_List, "[", ,8:12, FUN = mean)
for (i in seq_along(Epoch_List_Raw)){
  
    this_epoch=Epoch_List_Raw[[i]][,!(T2:ExtraTrials)]
    this_epoch$Theta=(rowMeans(Epoch_List_Raw[[i]][,T4:T8]))
    this_epoch$Alpha= (rowMeans(Epoch_List_Raw[[i]][,A8:A12]))
    this_epoch$FzMaxAmp=Epoch_List_Raw[[i]]$FzMaxAmp
    this_epoch$PzMaxAmp=Epoch_List_Raw[[i]]$PzMaxAmp
    this_epoch$F3MaxAmp=Epoch_List_Raw[[i]]$F3MaxAmp
    this_epoch$P3MaxAmp=Epoch_List_Raw[[i]]$P3MaxAmp
    this_epoch$F4MaxAmp=Epoch_List_Raw[[i]]$F4MaxAmp
    this_epoch$P4MaxAmp=Epoch_List_Raw[[i]]$P4MaxAmp
    Epoch_List[Epochs[i]] = list(this_epoch)
}

  # list over Threshold
  print(paste(Threshold, "uV was the threshold across all Epochs and participants"))

Over_T = list()
for (i in seq_along(Epoch_List)){

 this_Over_T=NULL
 this_Over_T=Epoch_List[[i]][FzMaxAmp>Threshold | PzMaxAmp>Threshold | F3MaxAmp>Threshold | P3MaxAmp>Threshold | F4MaxAmp>Threshold | P4MaxAmp>Threshold]
 Over_T[i] = list(this_Over_T)
  }
  
  Conditions =c('Eyes_Open', 'Eyes_Closed', 'Countdown', 'LISN', 'Baseline Total','24dB', '20dB', '16dB',
                     '12dB', '8dB', '4dB', '0dB','WIN Total')
  
EpochRM=setNames(data.frame(matrix(ncol = 3, nrow = 13)), c("Condition", "Num_RM", "Percent"))
  EpochRM$Condition=Conditions
  EpochRM_List = list()
  
  for (O_i in seq_along(Over_T)){

    this_EpochRM = EpochRM
  
    for (C_i in seq_along(Conditions)){
    this_EpochRM[C_i,2] = nrow(dplyr::filter(Over_T[[O_i]], Condition ==Conditions[C_i]))
    this_EpochRM[C_i,3] = this_EpochRM[C_i,2]/ (nrow(dplyr::filter(Epoch_List[[O_i]], Condition ==Conditions[C_i])))
    }
    
    this_EpochRM[5,2]=sum(this_EpochRM[1:4,2])
    this_EpochRM[13,2]=sum(this_EpochRM[6:12,2])
    this_EpochRM[13,3]=this_EpochRM[13,2]/(nrow(dplyr::filter(Epoch_List[[O_i]], Run %in% c("WIN_1", "WIN_2", "WIN_R1", "WIN_R2"))))
    EpochRM_List[O_i] = list(this_EpochRM)
    
  }
    

  # print table of WIN condition epochs removed

  RMtable=  kable(EpochRM_List[[1]][6:13,], row.names= F)%>%kable_styling(bootstrap_options = c(  "condensed"),full_width = F)%>%
  column_spec(1,color="black", bold=T)%>%
  row_spec(0:8,color = "black")
   print(RMtable)

 
     # Remove over Threshold   
  Channels = c('FzMaxAmp', 'PzMaxAmp', 'F3MaxAmp', 'P3MaxAmp', 'F4MaxAmp', 'P4MaxAmp')
  
  test = Epoch_List
   for (i in seq_along(Epoch_List)){
     Epoch_List[[i]]= Epoch_List[[i]][FzMaxAmp<Threshold & PzMaxAmp<Threshold & F3MaxAmp<Threshold & P3MaxAmp<Threshold &
                                        F4MaxAmp<Threshold & P4MaxAmp<Threshold]
   }
  


```


## Plots 


```{r, echo=FALSE, warning=FALSE, results='hide'}

 for (i in seq_along(Epoch_List)){

# Order the SNR levels for plot

Epoch_List[[i]]$Condition = gsub("dB", "", Epoch_List[[i]]$Condition) # remove dB

Epoch_List[[i]]$Condition <- factor(Epoch_List[[i]]$Condition, levels=c("24","20","16","12", "8", "4","0")) 
Epoch_List[[i]]$SNR_plot <- factor(Epoch_List[[i]]$Condition, levels=c("24","20","16","12", "8", "4","0"))
Epoch_List[[i]]$Response = factor(Epoch_List[[i]]$Response, levels = c("Cor", "InCor", "NoRes"))  
# EEG_Summary_Theta = as.data.frame( Epoch_List[[3]] %>% group_by(Condition) %>% summarise(Theta_M = mean(Theta), Theta_SD = sd(Theta)))
# EEG_Summary_Alpha = as.data.frame( Epoch_List[[3]] %>% group_by(Condition) %>% summarise(Alpha_M = mean(Alpha), Alpha_SD = sd(Alpha)))

# create the variable Paradigm 

#  which records whether the conditions were presented in sequential order
#  or in randomized order
Epoch_List[[i]]$Paradigm <- NA
Epoch_List[[i]]$Paradigm[Epoch_List[[i]]$Run %in% c("WIN_1", "WIN_2")] <- 0
Epoch_List[[i]]$Paradigm[Epoch_List[[i]]$Run %in% c("WIN_R1", "WIN_R2")] <- 1
# convert to factor and label the levels
Epoch_List[[i]]$Paradigm <- factor(Epoch_List[[i]]$Paradigm, labels=c("sequential", "randomized"))

Epoch_List[[i]]$SNR = scale(as.numeric(Epoch_List[[i]]$Condition), center = T, scale = F) # SNR is centered and continuous 

}

PC_df=PC_SR
PC_SR$Condition = gsub("dB", "", PC_SR$Condition) # add space
PC_df$Condition <- factor(PC_SR$Condition, levels=c("24","20","16","12", "8", "4","0")) 
PC_df$SNR_plot <- factor(PC_SR$Condition, levels=c("24","20","16","12", "8", "4","0")) 
PC_df$SNR = scale(as.numeric(PC_df$Condition), center = T, scale = F)# SNR is centered and continuous 
PC_df$Paradigm <- NA
PC_df$Paradigm[PC_df$Run %in% c("WIN_1", "WIN_2")] <- 0
PC_df$Paradigm[PC_df$Run %in% c("WIN_R1", "WIN_R2")] <- 1
# convert to factor and label the levels
PC_df$Paradigm <- factor(PC_df$Paradigm, labels=c("sequential", "randomized"))

# create Percent Correct variable
PC_df$PerCor = (PC_df$WIN_Score/5)*100

```


```{r, echo=FALSE, warning=FALSE, results='hide',dpi = 300, dev.args = list(png  = list(type = "cairo"))}
# PC plot
# PC_plot=
  ggplot(dplyr::filter(PC_df, Condition != "LISN"), aes(x=SNR_plot, y=PerCor, group = Group, shape = Group, color = Group))+
   geom_point(stat="summary", fun=mean, size = 5)+
   geom_line(stat="summary", fun=mean, size=2)+
  stat_summary( geom = "errorbar", fun.data = mean_se, position = "dodge", color="black",size=.8, width=.4)+
  scale_y_continuous(breaks=c(0, 20,40, 60, 80,100))+
   #labs(title = "Words Recognized over WIN SNR")+
    xlab("SIGNAL-TO-NOISE RATIO (dB)")+ ylab("WORDS RECOGNIZED \nIN % CORRECT")+
plot_finish
 
   
# print(PC_plot)
#ggsave(PC_plot, filename = "PC_plot.png", height = 4, width = 6, dpi = 300, type = "cairo")
```




```{r,  echo=FALSE, warning=FALSE, results='hide',dpi = 300, dev.args = list(png  = list(type = "cairo"))}
PC_plot=ggplot()+
  
  geom_point(data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_1'), aes(x=SNR_plot, y=PerCor),
             shape=21, fill = "black",stat="summary", fun=mean, size = 4)+
  geom_line (data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_1'), aes(x=SNR_plot, y=PerCor),
              stat="summary", fun=mean, group=10, linetype='solid', size=1.3)+
  stat_summary(data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_1'), aes(x=SNR_plot, y=PerCor),
                geom = "errorbar", fun.data = mean_se, position = "dodge", size=.7,width=.1)+
  
    
  geom_line (data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_2'), aes(x=SNR_plot, y=PerCor),
              stat="summary", fun=mean, group=10, linetype='solid', size=1.3)+
  stat_summary(data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_2'), aes(x=SNR_plot, y=PerCor),
                geom = "errorbar", fun.data = mean_se, position = "dodge", size=.7,width=.1)+
  geom_point(data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_2'), aes(x=SNR_plot, y=PerCor),
             shape=21,fill = "white",stat="summary", fun=mean, size = 4)+
  
     
  geom_line (data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_R1'), aes(x=SNR_plot, y=PerCor),
              stat="summary", fun=mean, group=10, linetype='solid',  size=1.3)+
  stat_summary(data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_R1'), aes(x=SNR_plot, y=PerCor),
                geom = "errorbar", fun.data = mean_se, position = "dodge", size=.7,width=.1)+ 
  geom_point(data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_R1'), aes(x=SNR_plot, y=PerCor),
             shape=24,fill = "black", stat="summary", fun=mean, size = 4)+
  
        
  geom_line (data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_R2'), aes(x=SNR_plot, y=PerCor),
              stat="summary", fun=mean, group=10, linetype='solid', size=1.3)+
  stat_summary(data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_R2'), aes(x=SNR_plot, y=PerCor),
                geom = "errorbar", fun.data = mean_se, position = "dodge", size=.7,width=.1)+
  geom_point(data=dplyr::filter(PC_df, Condition != "LISN", Run == 'WIN_R2'), aes(x=SNR_plot, y=PerCor),
             shape=24,fill = "white",stat="summary", fun=mean, size = 4)+
  
   
  theme_classic()+
   #labs(title = "Words Recognized over WIN SNR")+
    #scale_y_continuous(breaks=c(.20,.40, .60, .80,1),labels = scales::percent_format(accuracy = 1))+
  scale_y_continuous(breaks=c(20,40, 60, 80,100))+
    xlab("SIGNAL-TO-NOISE RATIO (dB)")+ ylab("WORDS RECOGNIZED \nIN % CORRECT")+
plot_finish
print(PC_plot)
```


### Self-Reported Listening Effort
```{r,  echo=FALSE, warning=FALSE, results='hide',dpi = 300, dev.args = list(png  = list(type = "cairo"))}
# attempt at PC and self-report plot
SelfReport_plot=ggplot()+

  geom_point(data=dplyr::filter(PC_df, Condition != "LISN"), aes(x=SNR_plot, y=RLE),
             shape=23,fill = "black", stat="summary", fun=mean, size = 4)+
  geom_line (data=dplyr::filter(PC_df, Condition != "LISN"), aes(x=SNR_plot, y=RLE),
              stat="summary", fun=mean, group=10, linetype='solid', size=1.3)+
  stat_summary(data=dplyr::filter(PC_df, Condition != "LISN"), aes(x=SNR_plot, y=RLE),
                geom = "errorbar", fun.data = mean_se, position = "dodge", size=.7,width=.2)+
  
    
  geom_line (data=dplyr::filter(PC_df, Condition != "LISN"), aes(x=SNR_plot, y=ILE),
              stat="summary", fun=mean, group=10, linetype='solid', size=1.3)+
  stat_summary(data=dplyr::filter(PC_df, Condition != "LISN"), aes(x=SNR_plot, y=ILE),
                geom = "errorbar", fun.data = mean_se, position = "dodge", size=.7,width=.2)+
  geom_point(data=dplyr::filter(PC_df, Condition != "LISN"), aes(x=SNR_plot, y=ILE),
             shape=23,fill = "white", stat="summary", fun=mean, size = 4)+

  scale_y_continuous (limits = c(0,9),breaks = (0:9))+
  theme_classic()+
 # labs(title = "Self-Reported Listening Effort Over WIN SNR")+
    xlab("SIGNAL-TO-NOISE RATIO (dB)")+ ylab("REQUIRED & INVESTED \nSELF-REPORTED EFFORT")+
plot_finish
   
print(SelfReport_plot)
```



## log transform within each subject:
 Theta, Alpha
```{r}
for (i in seq_along(Epoch_List)) {
  Epoch_List[[i]] =  Epoch_List[[i]]%>%dplyr::group_by(Sub,Run,Condition, SNR,SNR_plot)%>%dplyr::summarise(Theta =log((Theta)), Alpha = log((Alpha)))
}
```

```{r echo=FALSE}
Epoch_Win = c('Phrase' ,'Word' ,'Late')
Mycolors = c("Theta" = "dodgerblue2", "Alpha" = "red2")
# Mycolors = c("dodgerblue2", "red2", 'gray')
Freq_plot = NULL
 for (i in seq_along(Epoch_List)){
   # change alpha theta
       Freq_plot[[i]] = Epoch_List[[i]]%>%dplyr::select( Sub, Theta, Alpha, SNR_plot)%>%pivot_longer(cols = Theta:Alpha, 
                                           names_to = "Frequency",
                                      names_transform = list(Frequency = ~readr::parse_factor(.x, levels = c("Theta", "Alpha"))),
                                           values_to = "Power")
#######
# All_Freq_Plot=ggplot(data=dplyr::filter(Epoch_List[[i]], !is.na(SNR_plot)), aes(x=SNR_plot))+
print(ggplot(data=dplyr::filter(Freq_plot[[i]], !is.na(SNR_plot)), 
                     aes(x=SNR_plot, y = Power, color = Frequency, fill = Frequency, group = Frequency, shape = Frequency))+
        geom_line(aes(size=.8),stat="summary", fun=mean)+
        stat_summary(aes(size=.7,width=.2),geom = "errorbar", fun.data = mean_se, position = "dodge")+
        geom_point(aes( size = 4),stat="summary", fun=mean)+
                   
   scale_shape_manual(values=c(25, 22))+
   scale_color_manual (values = c("black", 'black'))+
   scale_fill_manual(values = Mycolors)+
         
  
      labs(title = paste(Epoch_Win[i],"- Frequency Power Over WIN SNR"))+
       xlab("SIGNAL-TO-NOISE RATIO (dB)")+   
    # ylab(bquote(PSD==10~"*"~log[10](µV^2*"/"~Hz)))
  # coord_cartesian(ylim=c(-.3, .4))+
plot_finish
)
}
```

### Plot individual theta/alpha, ILE, RLE, Win Score:

```{r echo=FALSE}
Mycolors = c("Late.Theta" = "dodgerblue2", "Alpha" = "red2")

# copy late theta to word epoch df:
  Epoch_List[[2]]$Late.Theta = Epoch_List[[3]]$Theta
    

#print(ggplot(data=dplyr::filter(Epoch_List[[i]], !is.na(SNR_plot), Sub ==  Subjects[Sub.i]), 
        Freq_plot = Epoch_List[[2]]%>%dplyr::select( Sub, Late.Theta, Alpha, SNR_plot)%>%pivot_longer(cols = Late.Theta:Alpha, 
                                           names_to = "Frequency",
                                      names_transform = list(Frequency = ~readr::parse_factor(.x, levels = c("Late.Theta", "Alpha"))),
                                           values_to = "Power")
i = 2
Subjects = unique(PC_df$Sub)
for (Sub.i in seq_along(Subjects)) {
  print(ggplot(data=dplyr::filter(PC_df, Condition != "LISN", Sub == Subjects[Sub.i]))+
  geom_point( aes(x=SNR_plot, y=RLE),
             shape=23,fill = "black", stat="summary", fun=mean, size = 4)+
  geom_line ( aes(x=SNR_plot, y=RLE),
              stat="summary", fun=mean, group=10, linetype='solid', size=1.3)+
  stat_summary( aes(x=SNR_plot, y=RLE),
                geom = "errorbar", fun.data = mean_se, position = "dodge", size=.7,width=.2)+
    geom_line ( aes(x=SNR_plot, y=ILE),
              stat="summary", fun=mean, group=10, linetype='solid', size=1.3)+
  stat_summary( aes(x=SNR_plot, y=ILE),
                geom = "errorbar", fun.data = mean_se, position = "dodge", size=.7,width=.2)+
  geom_point( aes(x=SNR_plot, y=ILE),
             shape=23,fill = "white", stat="summary", fun=mean, size = 4)+
  
   geom_point(aes(x=SNR_plot, y=(PerCor/10)), size=4, stat="summary", fun=mean)+
   geom_line(aes(x=SNR_plot, y=(PerCor/10)),stat="summary", fun=mean, group=10, size=2)+
  stat_summary(aes(x=SNR_plot, y=(PerCor/10)), geom = "errorbar", fun.data = mean_se, position = "dodge", color="black",size=.8, width=.4)+
  labs(title = paste(Subjects[Sub.i], Epoch_Win[i],"- Frequency Power Over WIN SNR"))+
  plot_finish)



#######
# All_Freq_Plot=ggplot(data=dplyr::filter(Epoch_List[[i]], !is.na(SNR_plot)), aes(x=SNR_plot))+
print(ggplot(data=dplyr::filter(Freq_plot, !is.na(SNR_plot),Sub ==  Subjects[Sub.i]), 
                     aes(x=SNR_plot, y = Power, color = Frequency, fill = Frequency, group = Frequency, shape = Frequency))+
        geom_line(aes(size=.8),stat="summary", fun=mean)+
        stat_summary(aes(size=.7,width=.2),geom = "errorbar", fun.data = mean_se, position = "dodge")+
        geom_point(aes( size = 4),stat="summary", fun=mean)+
                   
   scale_shape_manual(values=c(25, 22))+
   scale_color_manual (values = c("black", 'black'))+
   scale_fill_manual(values = Mycolors)+
         
  
      labs(title = paste(Subjects[Sub.i], Epoch_Win[i],"- Frequency Power Over WIN SNR"))+
       xlab("SIGNAL-TO-NOISE RATIO (dB)")+   
    # ylab(bquote(PSD==10~"*"~log[10](µV^2*"/"~Hz)))
  # coord_cartesian(ylim=c(-.3, .4))+
plot_finish
)

 }
```


### Combine EEG and Demo data

```{r, warning=FALSE}
# EEG_WIN_PTA=Epoch_List
EEG_WIN_PTA_ML = list()

for (i in seq_along(Epoch_List)){
  
#####
# First average Freq of each condition:
EEG_LE_Avg= Epoch_List[[i]]%>%dplyr::group_by(Sub,Run,Condition, SNR)%>%dplyr::summarise(Theta =mean((Theta)), Alpha = mean((Alpha)))
# merge data sets
EEG_PC_SR = merge(EEG_LE_Avg, PC_SR, all=T)

## collapse across Run
EEG_PC_SR%<>%group_by(Sub, Condition)%>% summarise_each(funs(mean(., na.rm = TRUE)))

# remove Run column
EEG_PC_SR=within(EEG_PC_SR,rm(Run))

# # Split Baseline from WIN 

EEG_WIN=EEG_PC_SR%>%dplyr::select(Sub,Condition, SNR,Theta, Alpha, ILE, RLE, WIN_Score) %>%
  dplyr::filter(Condition %in% c("24", "20", "16", "12", "8", "4", "0"))
EEG_WIN$SNR_plot <- factor(EEG_WIN$Condition, levels=c("24","20","16","12", "8", "4","0")) 


EEG_WIN$SNR_num = as.numeric(as.character( EEG_WIN$Condition))


#merge with PTA Demo
EEG_WIN_PTA_ML[[i]]=merge(EEG_WIN, PTA_Demo)
}

```

## Using Word Alpha and Late Theta
These had the strongest effects of SNR. 

```{r}
EEG.ml.dt = as.data.table(EEG_WIN_PTA_ML[[2]]) # just looking at Word epoch 
setkey(EEG.ml.dt, Sub) # set Subs as key
# # find and remove WIN Practice data"
# toRemove =  colnames(EEG.ml.dt[,grep('WIN_P_', names(EEG.ml.dt)), with = FALSE])# find
# set(EEG.ml.dt, , toRemove, NULL)# remove
EEG.ml.dt$Late.Theta = EEG_WIN_PTA_ML[[3]]$Theta # add late theta
EEG.ml.dt$Group = factor(EEG.ml.dt$Group, levels = c('YN', 'ONH', 'OHL'))
# Many char columns should be numeric:
# Get all character columns
CharacterCols = EEG.ml.dt%>% select_if(is.character)%>%colnames()
CharacterCols = CharacterCols[c(8:length(CharacterCols))] # limit to everything after Demographics
EEG.ml.dt[, (CharacterCols) := lapply(.SD, as.numeric), .SDcols = CharacterCols] # change the selected columns to numeric

```

# Steps to find peak LE SNR, Alternate using self-report: 
1) Find SNRs that have >= 70%
2) Within >= 70% SNRs, find the SNR that generates the largest first derivative of Alpha, Late Theta, ILE and RLE
3) Within >= 70% SNRs, find the SNR that generates peak (positive) of Alpha and Late Theta
4) Mean of those SNRs is the predicted SNR to have peak LE. 

#  Steps to define peak LE SNR (outcome), using EEG measures:{#ML_Context}
1) Find SNRs that have >= 70%
2) Within >= 70% SNRs, find the SNR that generates the largest first derivative of Alpha and Late Theta
3) Within >= 70% SNRs, find the SNR that generates peak (positive) of Alpha and Late Theta
4) Mean of those SNRs is the predicted SNR to have peak LE. 

```{r}
EEG.ml = as.data.frame(EEG.ml.dt)
Per.ceiling = 3.5
SNRs = seq(24,0, -4) #to select the SNRs analyze  eg. SNRs[1:which(SNRs == Ceiling.SNR)]
SNRs.d1 = seq(20,0, -4)
EEG.ml$Ceiling.SNR = NA
EEG.ml$Alpha.d1 = NA
EEG.ml$LTheta.d1 = NA
EEG.ml$Alpha.peak = NA
EEG.ml$LTheta.peak = NA
EEG.ml$ILE.d1 = NA
EEG.ml$RLE.d1 = NA
EEG.ml$Mean.LE.SNR = NA
for (Sub.i in seq_along(Subjects)) {
  Ceiling.SNR = NULL
  Alpha.d1 = NULL
  LTheta.d1 = NULL
  Alpha.peak = NULL 
  LTheta.peak = NULL
  ILE.d1 = NULL
  RLE.d1 = NULL
  # find the lowest SNR with >= Per.ceiling
  Ceiling.SNR = min(as.numeric(as.character(unique(EEG.ml[with(EEG.ml,Sub == Subjects[Sub.i] & WIN_Score >=Per.ceiling),]$Condition))))
  EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$Ceiling.SNR = Ceiling.SNR # save to df
  id = SNRs[1:which(SNRs == Ceiling.SNR)] # get seq of ceiling performance SNRs
  if (length(id) != 1){ # if there is only one condition, there is no derivative, just name that condition. 
  # find frequency power first derivative with largest change
    Alpha.d1 = which.max(abs(diff(EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i] & SNR_num %in% id),]$Alpha)))# diff(x)=1 so no need to divide
    EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$Alpha.d1 = SNRs.d1[Alpha.d1]
    
    LTheta.d1 = which.max(abs(diff(EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i] & SNR_plot %in% id),]$Late.Theta)))
    EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$LTheta.d1 = SNRs.d1[LTheta.d1]
    
  # find self-report first derivative with largest change
    ILE.d1 = which.max(abs(diff(EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i] & SNR_num %in% id),]$ILE)))
    EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$ILE.d1 = SNRs.d1[ILE.d1]

    RLE.d1 = which.max(abs(diff(EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i] & SNR_plot %in% id),]$RLE)))
    EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$RLE.d1 = SNRs.d1[RLE.d1]
  }else{ # just name that condition. 
     EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$Alpha.d1 = id
     EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$LTheta.d1 = id
     EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$ILE.d1 = id
     EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$RLE.d1 = id
  }
  
  # find peak frequency power 
  Alpha.peak = which.max(EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i] & SNR_num %in% id),]$Alpha)
  EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$Alpha.peak = SNRs[Alpha.peak]
     
  LTheta.peak = which.max(EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i] & SNR_plot %in% id),]$Late.Theta)
  EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$LTheta.peak = SNRs[LTheta.peak]

  # Mean SNR of all above predictors:
  # EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$Mean.LE.SNR = round(with(EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),], 
  #                                                                  mean(c(Alpha.d1, LTheta.d1, Alpha.peak, LTheta.peak, ILE.d1, RLE.d1))), digits = 1)
  ### Alternate outcome base on EEG measures only
    EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),]$Mean.LE.SNR = round(with(EEG.ml[with(EEG.ml, Sub == Subjects[Sub.i]),], 
                                                                   mean(c(Alpha.d1, LTheta.d1, Alpha.peak, LTheta.peak))), digits = 1)
}
 table(EEG.ml$Mean.LE.SNR)
# Histo = hist(EEG.ml$Mean.LE.SNR)
# Histo[['counts']] = Histo[['counts']]/7 # divide by number of conditions to get counts of subjects, data is in long format
# plot(Histo,  xlab = "Mean of Predicted peak LE SNR", main = 'Count of subject in each predicted peak LE SNR', col = 'gray')

# by Group
 print(EEG.ml%>%ggplot(aes(x = Mean.LE.SNR, fill = Group ))+
 geom_histogram(aes(y = ..count../7), bins = 25, color="#e9ecef", alpha=0.6, position = 'identity')+
   
    scale_y_continuous (limits = c(0,8),breaks = (0:8))+
   # scale_x_continuous ()+
   scale_x_reverse(limits = c(25,7),breaks = (25:7))+
   theme_classic()+
   ylab('Participant Count'))
quantile(EEG.ml$Mean.LE.SNR)

```

### Catigories of Effort:
Minimal = 6 - 4
Normal = 14 -18
Above Normal = 18 - 24

### Missing additional hearing eval data in YN, therefore the data allows two approaches:
1) YN (n=34), OHL (n=10), ONH (n=8), using hearing thresholds and self-report, no EEG data
2) OHL, ONH, using all hearing eval data without the YN. This would be a future direction.

Will try first approach here. 


```{r}
# Fix Col name issue
setnames(EEG.ml, "Veteran?","Veteran")

# remove unwanted variables:
additional.HE = c('TFI', 'Left SRT', 'Right SRT', 'Left Word Rec', 'Right Word Rec', 'SII Left', 'SII Right' )
toRemove = c('SNR', 'SAE', 'PCE', "SNR_num","SNR_inver",'SNR_plot',"Neuroeconomic", "Proxy1", "WIN Test Level", 'MoCA Extra Point?', "Adj_MoCA", 'Handedness', 'Veteran', 'Gender',  "Theta","Late.Theta", "Alpha")
toRemove = c(additional.HE, toRemove, 'Ethnicity', 'Race') # Have to remove Ethnicity, only one non-white
EEG.ml.wider = EEG.ml[!(colnames(EEG.ml) %in% toRemove)]
# pivot wider:
EEG.ml.wider = pivot_wider (EEG.ml.wider,
             names_from = Condition,
             values_from = c(ILE, RLE, WIN_Score))
print('Which variables have NAs?')
apply(is.na(EEG.ml.wider), 2, which)  
```

```{r}
#create table for model result comparison
Model.Results = data.frame(Model = c('RF.All', 'LM.All', 'RF.Lasso', 'LM.Lasso', 'RF.EL', 'LM.EL', 'RF.Inter.EL', 'LM.Inter.EL'), 
                           R2 = rep(NA, 8), RMSE = rep(NA, 8))
```


### Test Train split
Currently I have a set group for the test train split. The set test group was from a random sample with 2 from the ONH and OHL groups. 
```{r}

# test.obs = sample(sub.names, size = length(sub.names)*.2 ) # take 20% of data set obs
# Need random sample with 2 from both older groups: 
test.obs = c("YN_10","ONH_17", "OHL_04", "OHL_07", "YN_39" , "YN_09" , "YN_17",  "YN_13",  "YN_36" , "ONH_07")

# Split:
EEG.db = list()
EEG.db[['train']] =  dplyr::filter(EEG.ml.wider, !Sub %in% test.obs)# remove Subs in test.obs
EEG.db[['test']] =  dplyr::filter(EEG.ml.wider, Sub %in% test.obs)# keep Subs in test.obs
```

Test sample includes: "YN_10","ONH_17", "OHL_04", "OHL_07", "YN_39" , "YN_09" , "YN_17",  "YN_13",  "YN_36" , "ONH_07"

### Test model for multicolliniearty 

```{r}
# Unwanted predictors for model
Unwanted = c( 'Sub', 'Group',"Alpha.d1", "LTheta.d1","Alpha.peak","LTheta.peak")


# Build linear model
model1 <- lm(Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][!colnames(EEG.db[['train']]) %in% Unwanted])
# Make predictions
predictions <- model1 %>% predict(EEG.db[['test']][!colnames(EEG.db[['test']]) %in% Unwanted])

# Model performance
data.frame(
  RMSE = RMSE(predictions, EEG.db[['test']][!colnames(EEG.db[['test']]) %in% Unwanted]$Mean.LE.SNR),
  R2 = R2(predictions,EEG.db[['test']][!colnames(EEG.db[['test']]) %in% Unwanted]$Mean.LE.SNR)
)

try(
car::vif(model1))
# collect alias variables
alias_list = rownames(alias(model1)[["Complete"]])

# test new model without alias variables: 
Unwanted = c(Unwanted, alias_list)

# Build linear model
model1 <- lm(Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][!colnames(EEG.db[['train']]) %in% Unwanted])

try(
car::vif(model1))
## # still trouble with colinearity remove win scores? 
Unwanted = c(Unwanted, 'WIN_Score_24', 'WIN_Score_20'  )

# Build linear model
model1 <- lm(Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][!colnames(EEG.db[['train']]) %in% Unwanted])

try(
car::vif(model1))
```

### Try Random Forest with all variables

```{r, fig.width=7, fig.height=8 }

# Filter out unwanted variables: 
  EEG.model = randomForest(formula = Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][!colnames(EEG.db[['train']]) %in% Unwanted], ntree = 4000, importance=TRUE)



  # model output
  EEG.model
  # plot tree error
  plot(EEG.model)

varImpPlot(EEG.model)

pred = predict(EEG.model, EEG.db[['test']][!colnames(EEG.db[['test']]) %in% Unwanted])

  # Find Test R^2
  Model.Results[1, 2:3] = find.r2(pred, EEG.db[['test']]$Mean.LE.SNR )
  
print('Compare to linear regression')
EEG.model.lm = lm(formula = Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][!colnames(EEG.db[['train']]) %in% Unwanted])
  # model output
  EEG.model.lm
    print('VIF of lm')
car::vif(EEG.model.lm)

pred = predict(EEG.model.lm, EEG.db[['test']][!colnames(EEG.db[['test']]) %in% Unwanted])

  # Find Test R^2
   Model.Results[2, 2:3] = find.r2(pred, EEG.db[['test']]$Mean.LE.SNR )

```

### Regression tree output explainer: 
%IncMSE: hold the model constant except for one variable. Using the values of that variable in the data set, randomly (permute) them in the model and get the OOB (out of bag CV) error. So, %IncMSE tells us how much randomizing the data for individual variables effects the predictive power of the model. The more it effects the outcome, the more important the variable to OOB prediction, the higher the %IncMSE.

IncNodePurity: at each split, you can calculate how much this split reduces node impurity (for regression trees, the difference between RSS before and after the split). This is summed over all splits for that variable, over all trees.

### Examine variables to find most important, multicolliniearty is major issue: 

Look at lasso and parameter tuning: https://uc-r.github.io/random_forests

First look at Lasso to see what parameters are useful: https://www.statology.org/lasso-regression-in-r/

To perform lasso regression, we’ll use functions from the glmnet package. This package requires the response variable to be a vector and the set of predictor variables to be of the class data.matrix.

Next, we’ll use the glmnet() function to fit the lasso regression model and specify alpha=1.
Note that setting alpha equal to 0 is equivalent to using ridge regression and setting alpha to some value between 0 and 1 is equivalent to using an elastic net. 

To determine what value to use for lambda, we’ll perform k-fold cross-validation and identify the lambda value that produces the lowest test mean squared error (MSE).

Note that the function cv.glmnet() automatically performs k-fold cross validation using k = 6 folds.

### Lasso Regression
```{r}
# Add output variable to Unwanted variables list for lasso


  # training data set:
  Mean.LE.SNR.train = EEG.db[['train']]$Mean.LE.SNR # make output variable
  Predictors.train = data.matrix(EEG.db[['train']][!colnames(EEG.db[['train']]) %in% c('Mean.LE.SNR', Unwanted)]) # make parameters matrix
  # test data set
  Mean.LE.SNR.test = EEG.db[['test']]$Mean.LE.SNR # make output variable
  Predictors.test = data.matrix(EEG.db[['test']][!colnames(EEG.db[['test']]) %in% c('Mean.LE.SNR', Unwanted)]) # make parameters matrix
  
  lambda_seq = 10^seq(2, -2, by = -.1)
  EEG.lasso = cv.glmnet(Predictors.train, Mean.LE.SNR.train , alpha = 1, nfolds = 6, lambda = lambda_seq ) # run 5-fold cross-validation of lasso
  # # Find optimal lambda value to minimize MSE
  # print(EEG.lasso$lambda.min)
  # plot test MSE by lambda value
  plot( EEG.lasso )
  
  # find coefficients of best model:
  best_model = glmnet(Predictors.train, Mean.LE.SNR.train , alpha = 1, lambda = EEG.lasso$lambda.min)
  EEG.best.model.coeff=  coef(best_model)
  EEG.best.model.coeff
  # print(plot(best_model, xvar = "lambda"))
  # Rebuilding the model with best lambda value identified
  pred <- predict( best_model, s = EEG.lasso$lambda.min, newx = Predictors.test)
  
  # Find R^2
  find.r2(pred, Mean.LE.SNR.test, output = FALSE)

lasso_var = rownames(EEG.best.model.coeff)[EEG.best.model.coeff[,1] !=0]
lasso_var = lasso_var[-1] # remove 'intercept' from vector
print(lasso_var)
```


### Try Random Forest with just Lasso variables

```{r}

  EEG.model = randomForest(formula = Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][colnames(EEG.db[['train']]) %in% c('Mean.LE.SNR', lasso_var)], ntree = 2000, importance=TRUE)

  # model output
  EEG.model
  # plot tree error
  plot(EEG.model)

varImpPlot(EEG.model)

pred = predict(EEG.model, EEG.db[['test']][colnames(EEG.db[['test']]) %in% c('Mean.LE.SNR', lasso_var)])

  # Find Test R^2
  Model.Results[3, 2:3] = find.r2(pred, EEG.db[['test']]$Mean.LE.SNR)

print('Compare to linear regression')
EEG.model.lm = lm(formula = Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][colnames(EEG.db[['train']]) %in% c('Mean.LE.SNR', lasso_var)])
  # model output
  EEG.model.lm
  print('VIF of lm')
car::vif(EEG.model.lm)

pred = predict(EEG.model.lm, EEG.db[['test']][colnames(EEG.db[['test']]) %in% c('Mean.LE.SNR', lasso_var)])

  # Find Test R^2
  Model.Results[4, 2:3] = find.r2(pred, EEG.db[['test']]$Mean.LE.SNR )
```

 
### Elastic-Net, avg of 50 interations

```{r}
# From https://daviddalpiaz.github.io/r4sl/elastic-net.html
# Train n= 42, 6 folds give 7 results
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
set.seed(42)
cv_6 = trainControl(method = "cv", number = 6)
el.net.result = data.table(alpha = as.numeric(rep(NA, 50)), lambda = as.numeric(rep(NA, 50)), 
                           RMSE = as.numeric(rep(NA, 50)), Rsquared = as.numeric(rep(NA, 50)))
for (i in 1:50){
el.net = train(Mean.LE.SNR ~., data = EEG.db[['train']][!colnames(EEG.db[['train']]) %in% Unwanted], 
               method = "glmnet", trControl = cv_6)
el.net.result[i,]= get_best_result(el.net)[1,1:4]
}

summary(el.net.result)
print('how often was alpha == 1?')
 sum(el.net.result$alpha == 1)/50
 print('Already did a lasso, lets try alpha = .055')
 # New model with best results
  # find coefficients of best model:
  best_model = glmnet(Predictors.train, Mean.LE.SNR.train , alpha = .55, lambda = 0.5768)
  EEG.best.model.coeff=  coef(best_model)
  print( EEG.best.model.coeff)
  # print(plot(best_model, xvar = "lambda"))
  # Rebuilding the model with best lambda value identified
  pred <- predict( best_model, newx = Predictors.test)
  
  # Find R^2

  find.r2(pred, Mean.LE.SNR.test, output = FALSE)

  plot_result = data.frame( Mean.LE.SNR.test,  pred = as.vector(pred))
  ggplot(data = plot_result, aes(x = Mean.LE.SNR.test, y = pred))+
    geom_jitter(size = 3, alpha = .7)+
    geom_smooth(method = 'lm')
  
  el.net.var = rownames(EEG.best.model.coeff)[EEG.best.model.coeff[,1] !=0]
  el.net.var = el.net.var[-1] # remove 'intercept' from vector
  print(el.net.var)
                        
```

### Try random forest with elastic net variables
```{r}

EEG.model = randomForest(formula = Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][colnames(EEG.db[['train']]) %in% c('Mean.LE.SNR', el.net.var)], ntree = 4000, importance=TRUE)



  # model output
  EEG.model
  # plot tree error
  plot(EEG.model)

  varImpPlot(EEG.model)

pred = predict(EEG.model, EEG.db[['test']][colnames(EEG.db[['test']]) %in% c('Mean.LE.SNR', el.net.var)])

  # Find Test R^2
  Model.Results[5, 2:3] = find.r2(pred, EEG.db[['test']]$Mean.LE.SNR)
   plot_result = data.frame( Mean.LE.SNR.test,  pred = as.vector(pred))
  ggplot(data = plot_result, aes(x = Mean.LE.SNR.test, y = pred))+
  geom_jitter(size = 3, alpha = .7)+
    geom_smooth(method = 'lm')
  
print('Compare to linear regression')
EEG.model.lm = lm(formula = Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][colnames(EEG.db[['train']]) %in% c('Mean.LE.SNR', el.net.var)])
  # model output
  EEG.model.lm
  print('VIF of lm')
car::vif(EEG.model.lm)

pred = predict(EEG.model.lm, EEG.db[['test']][colnames(EEG.db[['test']]) %in% c('Mean.LE.SNR', el.net.var)])

  # Find Test R^2
 Model.Results[6, 2:3] =  find.r2(pred, EEG.db[['test']]$Mean.LE.SNR )
 
```

### Now try a larger model search with all interactions and 10 alpha values, avg of 50 interations.

```{r, warning=FALSE}
set.seed(42)
cv_6 = trainControl(method = "cv", number = 6)
el.net.result = data.table(alpha = as.numeric(rep(NA, 50)), lambda = as.numeric(rep(NA, 50)), 
                           RMSE = as.numeric(rep(NA, 50)), Rsquared = as.numeric(rep(NA, 50)))
for (i in 1:50){
el.net = train(Mean.LE.SNR ~.^2, data = EEG.db[['train']][!colnames(EEG.db[['train']]) %in% Unwanted], 
               method = "glmnet", trControl = cv_6, tuneLength = 10)
el.net.result[i,]= get_best_result(el.net)[1,1:4]
}

summary(el.net.result)
best.el.net = el.net.result[which.min(el.net.result$RMSE),]

 # New model with best results
  # find coefficients of best model:
  best_model = glmnet(Predictors.train, Mean.LE.SNR.train , alpha = best.el.net$alpha, lambda = best.el.net$lambda)
  EEG.best.model.coeff=  coef(best_model)
  print( EEG.best.model.coeff)
  # print(plot(best_model, xvar = "lambda"))
  # Rebuilding the model with best lambda value identified
  pred <- predict( best_model, newx = Predictors.test)

  # Find R^2
  find.r2(pred, Mean.LE.SNR.test, output = FALSE)

  plot_result = data.frame( Mean.LE.SNR.test,  pred = as.vector(pred))
  ggplot(data = plot_result, aes(x = Mean.LE.SNR.test, y = pred))+
    geom_jitter(size = 3, alpha = .7)+
    geom_smooth(method = 'lm')
  
  el.net.inter.var = rownames(EEG.best.model.coeff)[EEG.best.model.coeff[,1] !=0]
  el.net.inter.var = el.net.inter.var[-1] # remove 'intercept' from vector
  print(el.net.inter.var)
  

```

### Try random forest with larger model elastic net variables
```{r}

EEG.model = randomForest(formula = Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][colnames(EEG.db[['train']]) %in% c('Mean.LE.SNR', el.net.inter.var)], ntree = 4000, importance=TRUE)



  # model output
  EEG.model
  # plot tree error
  plot(EEG.model)

  varImpPlot(EEG.model)

pred = predict(EEG.model, EEG.db[['test']][colnames(EEG.db[['test']]) %in% c('Mean.LE.SNR', el.net.inter.var)])

  # Find Test R^2
 Model.Results[7, 2:3] = find.r2(pred, EEG.db[['test']]$Mean.LE.SNR)
   plot_result = data.frame( Mean.LE.SNR.test,  pred = as.vector(pred))
  ggplot(data = plot_result, aes(x = Mean.LE.SNR.test, y = pred))+
  geom_jitter(size = 3, alpha = .7)+
    geom_smooth(method = 'lm')

print('Compare to linear regression')
EEG.model.lm = lm(formula = Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][colnames(EEG.db[['train']]) %in% c('Mean.LE.SNR', el.net.inter.var)])
  # model output
  EEG.model.lm
  print('VIF of lm')
car::vif(EEG.model.lm)

pred = predict(EEG.model.lm, EEG.db[['test']][colnames(EEG.db[['test']]) %in% c('Mean.LE.SNR', el.net.inter.var)])

  # Find Test R^2
 Model.Results[8, 2:3] = find.r2(pred, EEG.db[['test']]$Mean.LE.SNR )
```

## Compare Model Results
```{r}

kable(Model.Results, row.names= F)%>%kable_styling(bootstrap_options = c(  "condensed"),full_width = F)%>%
  column_spec(1,color="black", bold=T)%>%
  row_spec(0:nrow(Model.Results),color = "black")
print(paste('Model with lowest RMSE = ', Model.Results[which.min(Model.Results$RMSE),1]))

```

### Fine tune model:
Parameters in tuneRF function
The stepFactor specifies at each iteration, mtry is inflated (or deflated) by this value
The improve specifies the (relative) improvement in OOB error must be by this much for the search to continue
The trace specifies whether to print the progress of the search
The plot specifies whether to plot the OOB error as function of mtry

```{r}
#From: https://www.listendata.com/2014/11/random-forest-with-r.html

mtry <- tuneRF(EEG.db[['train']][colnames(EEG.db[['train']]) %in% lasso_var],
               EEG.db[['train']]$Mean.LE.SNR, ntreeTry=3500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
```

 Lets look at what is the best setting for the test data: 

```{r}
mtry.list = 2:length(lasso_var)
Avg.Result = cbind.data.frame( mtry.list, Avg.RMSE = rep(NA,length(mtry.list)), R2 = rep(NA, length(mtry.list)))
lasso_var.out =  c('Mean.LE.SNR',lasso_var)
ntree.best = 2500
# Let's speed things up with a data.table:
EEG.db.train = as.data.table(EEG.db[['train']])
EEG.db.test = as.data.table(EEG.db[['test']])
for (i in 1:length(mtry.list)) {
  R_2 = NULL
  RMSE.Result = NULL
  for (ii in 1:50) {
     EEG.model = randomForest(formula = Mean.LE.SNR ~ ., 
                         data = EEG.db.train[,..lasso_var.out], ntree = ntree.best, mtry = mtry.list[i], importance=TRUE)
pred = predict(EEG.model, EEG.db.test[,..lasso_var.out])

  # Find Test R^2

 R_2[ii] = find.r2(pred, EEG.db.test$Mean.LE.SNR, plot = FALSE, text = FALSE)[1]
 RMSE.Result[ii] = find.r2(pred, EEG.db.test$Mean.LE.SNR, plot = FALSE, text = FALSE)[2]
  }
 Avg.Result[i,2] = mean(RMSE.Result)
 Avg.Result[i,3] = mean(R_2)
 
  
} 

kable(Avg.Result, row.names= F)%>%kable_styling(bootstrap_options = c(  "condensed"),full_width = F)%>%
  column_spec(1,color="black", bold=T)%>%
  row_spec(0:nrow(Avg.Result),color = "black")
print('Model with lowest RMSE = ')
Avg.Result[which.min(Avg.Result$Avg.RMSE),]

mtry.best = Avg.Result[which.min(Avg.Result$Avg.RMSE),]$mtry.list
print(mtry.best)
```

## Now run the model with the best of trees and variables to try

```{r}

EEG.model = randomForest(formula = Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][colnames(EEG.db[['train']]) %in% lasso_var.out], ntree = ntree.best, mtry = mtry.best, importance=TRUE)

  # model output
  EEG.model
  # plot tree error
  plot(EEG.model)


# plot(EEG.model$importance)

varImpPlot(EEG.model)

pred = predict(EEG.model, EEG.db[['test']][colnames(EEG.db[['test']]) %in% lasso_var.out])

  # Find Test R^2
  find.r2(pred, EEG.db[['test']]$Mean.LE.SNR, output = FALSE )
 

```

### Compare to linear model

```{r}
EEG.lm.model = lm(Mean.LE.SNR ~ ., 
                         data = EEG.db[['train']][colnames(EEG.db[['train']]) %in% lasso_var.out])

  # model output
  EEG.lm.model
  # plot tree error
  plot(EEG.lm.model)

pred = predict(EEG.lm.model, EEG.db[['test']][colnames(EEG.db[['test']]) %in% lasso_var.out])

  # Find Test R^2
find.r2(pred, EEG.db[['test']]$Mean.LE.SNR, output = FALSE )

```


## Xgboost approach
  https://www.kaggle.com/rtatman/machine-learning-with-xgboost-in-r
https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html
  https://analyticsindiamag.com/complete-guide-to-xgboost-with-implementation-in-r/
  https://towardsdatascience.com/implementing-an-xgboost-model-in-r-59ee1892be2f

```{r}
EEG.db.train = as.data.table(EEG.db[['train']])
EEG.db.test = as.data.table(EEG.db[['test']])
# grouping variables need to be 'one hot' 
# find factor variables
colnames(Filter(is.factor, EEG.db.train))
Group.Matrix.train = model.matrix(~Group-1, EEG.db.train)
Group.Matrix.test = model.matrix(~Group-1, EEG.db.test)
# remove group variable
EEG.db.train[,Group :=NULL]
EEG.db.test[,Group := NULL]

# merge 
EEG.db.train = cbind(EEG.db.train, Group.Matrix.train)
EEG.db.test = cbind(EEG.db.test, Group.Matrix.test)

# get label, out outcome variable and make it separate
train.label = EEG.db.train$Mean.LE.SNR
EEG.db.train[, Mean.LE.SNR :=NULL]
test.label = EEG.db.test$Mean.LE.SNR
EEG.db.test[, Mean.LE.SNR :=NULL]

# change into Dmatrix, remove all non-numberic columns (Sub)
EEG.dm.train = xgb.DMatrix(data = as.matrix(EEG.db.train[,-1]), label = train.label)# -1 to remove Sub
EEG.dm.test = xgb.DMatrix(data = as.matrix(EEG.db.test[,-1]), label = test.label)
```
## Xgboost with all predictors

```{r}
# train model
modelxg = xgboost(data = EEG.dm.train,
                  max.depth = 2, 
                  nthread = 3,
                  nrounds = 10)

pred = predict(modelxg, EEG.dm.test)

find.r2(pred, test.label, output = FALSE )

importance_matrix <- xgb.importance(model = modelxg)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)

```

## Xgboost without EEG predictors

```{r}
#remove EEG vars
EEG.db.train[, c("Alpha.d1","LTheta.d1","Alpha.peak","LTheta.peak" ) :=NULL]
EEG.db.test[, c("Alpha.d1","LTheta.d1","Alpha.peak","LTheta.peak" ) :=NULL]

# change into Dmatrix, remove all non-numberic columns (Sub)
EEG.dm.train = xgb.DMatrix(data = as.matrix(EEG.db.train[,-1]), label = train.label)
EEG.dm.test = xgb.DMatrix(data = as.matrix(EEG.db.test[,-1]), label = test.label)
```

```{r}

# train model
modelxg = xgboost(data = EEG.dm.train,
                  max.depth = 2, 
                  nthread = 3,
                  booster = 'gbtree',
                  nrounds = 188) # max.depth = 2, nrounds = 188, min MSRE = 2.586

pred = predict(modelxg, EEG.dm.test)

# R2 of test data
find.r2(pred, test.label, output = FALSE )

# importance of variables
importance_matrix <- xgb.importance(model = modelxg)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)

# # plot tree
# xgb.plot.tree(model = modelxg)

```

Nice turning grid example:
https://shengyg.github.io/repository/machine%20learning/2017/02/25/Complete-Guide-to-Parameter-Tuning-xgboost.html
https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/


## Xgboost use only clinic measures

```{r}
#limit to only clinic vars
EEG.db.train.clinic = EEG.db.train[, c("Sub", "Age","Education","MoCA","L250","L500","L1000","L2000","L3000", "L4000","L6000",
                 "L8000","R250","R500","R1000","R2000","R3000","R4000", "R6000","R8000","PTA","HF_PTA" )]
EEG.db.test.clinic = EEG.db.test[, c("Sub", "Age","Education","MoCA","L250","L500","L1000","L2000","L3000", "L4000","L6000",
                 "L8000","R250","R500","R1000","R2000","R3000","R4000", "R6000","R8000","PTA","HF_PTA" )]

# change into Dmatrix, remove all non-numberic columns (Sub)
EEG.dm.train = xgb.DMatrix(data = as.matrix(EEG.db.train.clinic[,-1]), label = train.label)
EEG.dm.test = xgb.DMatrix(data = as.matrix(EEG.db.test.clinic[,-1]), label = test.label)
```

```{r}

# train model
modelxg = xgboost(data = EEG.dm.train,
                  max.depth = 2, 
                  nthread = 3,
                  booster = 'gbtree',
                  nrounds = 260) # max.depth = 2, nrounds = 254, min MSRE = 3.012

pred = predict(modelxg, EEG.dm.test)

# R2 of test data
find.r2(pred, test.label, output = FALSE )

# importance of variables
importance_matrix <- xgb.importance(model = modelxg)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)

# # plot tree
# xgb.plot.tree(model = modelxg)

```

## Catigorical approach
```{r}
print(EEG.ml%>%ggplot(aes(x =Mean.LE.SNR, fill = Group ))+
 geom_histogram(bins = 25, color="#e9ecef", alpha=0.6, position = 'identity'))
```


### Catigories of Effort:
Minimal = 6 - 14
Normal = 14 -18
Above Normal = 18 - 24

```{r}
EEG.db.train = as.data.table(EEG.db[['train']])
EEG.db.test = as.data.table(EEG.db[['test']])

# grouping variables need to be 'one hot' 
# find factor variables
colnames(Filter(is.factor, EEG.db.train))
Group.Matrix.train = model.matrix(~Group-1, EEG.db.train)
Group.Matrix.test = model.matrix(~Group-1, EEG.db.test)

# remove group variable
EEG.db.train[,Group :=NULL]
EEG.db.test[,Group := NULL]

# merge 
EEG.db.train = cbind(EEG.db.train, Group.Matrix.train)
EEG.db.test = cbind(EEG.db.test, Group.Matrix.test)

# Make grouping variables for categories of effort 
# Effort categories: 0= below normal, 1 = normal, 2 = above normal
EEG.db.train$Cat_Effort = as.numeric(ifelse(EEG.db.train$Mean.LE.SNR <= 14, '0',
                     ifelse(EEG.db.train$Mean.LE.SNR <= 18, '1', 
                     ifelse(EEG.db.train$Mean.LE.SNR < 24, '2','2'))))

EEG.db.test$Cat_Effort = as.numeric(ifelse(EEG.db.test$Mean.LE.SNR <= 14, '0',
                     ifelse(EEG.db.test$Mean.LE.SNR <= 18, '1', 
                     ifelse(EEG.db.test$Mean.LE.SNR < 24, '2','2'))))

# # Make grouping matrix
# train.label = model.matrix(~Cat_Effort-1, EEG.db.train)
# test.label = model.matrix(~Cat_Effort-1, EEG.db.test)

# Make label matrix
train.label = as.matrix(EEG.db.train$Cat_Effort)
test.label = as.matrix(EEG.db.test$Cat_Effort)

# # remove group variable
# EEG.db.train[,Cat_Effort :=NULL]
# EEG.db.test[,Cat_Effort := NULL]
# 
# # merge 
# EEG.db.train = cbind(EEG.db.train, Cat_Effort.Matrix.train)
# EEG.db.test = cbind(EEG.db.test, Cat_Effort.Matrix.test)

# get label, out outcome variable and make it separate
# train.label = EEG.db.train$Cat_Effort
EEG.db.train[, Cat_Effort :=NULL]
# test.label = EEG.db.test$Cat_Effort
EEG.db.test[, Cat_Effort :=NULL]

# Remove continuous outcome var
EEG.db.train[, Mean.LE.SNR :=NULL]
EEG.db.test[, Mean.LE.SNR :=NULL]

# change into Dmatrix, remove all non-numeric columns (Sub)
EEG.dm.train = xgb.DMatrix(data = as.matrix(EEG.db.train[,-1]), label = train.label)# -1 to remove Sub
EEG.dm.test = xgb.DMatrix(data = as.matrix(EEG.db.test[,-1]), label = test.label)
```


https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/

## Predict cagagory with EEG variables
https://rpubs.com/mharris/multiclass_xgboost
https://rstudio-pubs-static.s3.amazonaws.com/456044_9c275b0718a64e6286751bb7c60ae42a.html

```{r}
numberOfClasses <- length(unique(test.label))
xgb_params <- list("objective" = "multi:softprob",
                   "eval_metric" = "mlogloss",
                   "num_class" = numberOfClasses)
nround    <- 175 # number of rounds

# CV folds, how many folds
 # length(train.label)/3
cv.nfold  <- 7

# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
                   data = EEG.dm.train, 
                   nrounds = nround,
                   nfold = cv.nfold,
                   verbose = FALSE,
                   prediction = TRUE)

OOF_prediction <- data.frame(cv_model$pred) %>%
  mutate(max_prob = max.col(., ties.method = "last"),
         label = train.label + 1)
head(OOF_prediction)

confusionMatrix(factor(OOF_prediction$max_prob),
                factor(OOF_prediction$label),
                mode = "everything")

```

Hyper-parameter tuning

```{r}
# take parameters from CV  data
bst_model <- xgb.train(params = xgb_params,
                       data = EEG.dm.train,
                       nrounds = 1000, 
                       early_stopping_rounds = 10,
                       watchlist = list(val1 = EEG.dm.train, val2 = EEG.dm.test))



# Predict hold-out test set
test_pred <- predict(bst_model, newdata = EEG.dm.test)
test_prediction <- matrix(test_pred, nrow = numberOfClasses,
                          ncol=length(test_pred)/numberOfClasses) %>%
  t() %>%
  data.frame() %>%
  mutate(label = test.label + 1,
         max_prob = max.col(., "last"))
# confusion matrix of test set
confusionMatrix(factor(test_prediction$max_prob),
                factor(test_prediction$label),
                mode = "everything")

# get the feature real names
names <-  colnames(EEG.db.train[,-1])
# compute feature importance matrix
importance_matrix = xgb.importance(feature_names = names, model = bst_model)
head(importance_matrix)
xgb.ggplot.importance(importance_matrix)

```

## Predict cagagory without EEG variables
```{r}
#remove EEG vars
EEG.db.train[, c("Alpha.d1","LTheta.d1","Alpha.peak","LTheta.peak" ) :=NULL]
EEG.db.test[, c("Alpha.d1","LTheta.d1","Alpha.peak","LTheta.peak" ) :=NULL]

# change into Dmatrix, remove all non-numberic columns (Sub)
EEG.dm.train = xgb.DMatrix(data = as.matrix(EEG.db.train[,-1]), label = train.label)
EEG.dm.test = xgb.DMatrix(data = as.matrix(EEG.db.test[,-1]), label = test.label)
```

```{r}
numberOfClasses <- length(unique(test.label))
xgb_params <- list("objective" = "multi:softprob",
                   "eval_metric" = "mlogloss",
                   "num_class" = numberOfClasses)
nround    <- 175 # number of rounds

# CV folds, how many folds
 # length(train.label)/3
cv.nfold  <- 7

# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
                   data = EEG.dm.train, 
                   nrounds = nround,
                   nfold = cv.nfold,
                   verbose = FALSE,
                   prediction = TRUE)

OOF_prediction <- data.frame(cv_model$pred) %>%
  mutate(max_prob = max.col(., ties.method = "last"),
         label = train.label + 1)
head(OOF_prediction)

confusionMatrix(factor(OOF_prediction$max_prob),
                factor(OOF_prediction$label),
                mode = "everything")

```

Hyper-parameter tuning

```{r}
# take parameters from CV  data
bst_model <- xgb.train(params = xgb_params,
                       data = EEG.dm.train,
                       nrounds = 1000, 
                       early_stopping_rounds = 10,
                       watchlist = list(val1 = EEG.dm.train, val2 = EEG.dm.test))



# Predict hold-out test set
test_pred <- predict(bst_model, newdata = EEG.dm.test)
test_prediction <- matrix(test_pred, nrow = numberOfClasses,
                          ncol=length(test_pred)/numberOfClasses) %>%
  t() %>%
  data.frame() %>%
  mutate(label = test.label + 1,
         max_prob = max.col(., "last"))
# confusion matrix of test set
confusionMatrix(factor(test_prediction$max_prob),
                factor(test_prediction$label),
                mode = "everything")

# get the feature real names
names <-  colnames(EEG.db.train[,-1])
# compute feature importance matrix
importance_matrix = xgb.importance(feature_names = names, model = bst_model)
head(importance_matrix)
xgb.ggplot.importance(importance_matrix)

```

## Catagorical using only clinic measures

```{r}
#limit to only clinic vars
EEG.db.train.clinic = EEG.db.train[, c("Sub", "Age","Education","MoCA","L250","L500","L1000","L2000","L3000", "L4000","L6000",
                 "L8000","R250","R500","R1000","R2000","R3000","R4000", "R6000","R8000","PTA","HF_PTA" )]
EEG.db.test.clinic = EEG.db.test[, c("Sub", "Age","Education","MoCA","L250","L500","L1000","L2000","L3000", "L4000","L6000",
                 "L8000","R250","R500","R1000","R2000","R3000","R4000", "R6000","R8000","PTA","HF_PTA" )]

# change into Dmatrix, remove all non-numberic columns (Sub)
EEG.dm.train = xgb.DMatrix(data = as.matrix(EEG.db.train.clinic[,-1]), label = train.label)
EEG.dm.test = xgb.DMatrix(data = as.matrix(EEG.db.test.clinic[,-1]), label = test.label)
```


```{r}
numberOfClasses <- length(unique(test.label))
xgb_params <- list("objective" = "multi:softprob",
                   "eval_metric" = "mlogloss",
                   "num_class" = numberOfClasses)
nround    <- 175 # number of rounds

# CV folds, how many folds
 # length(train.label)/
cv.nfold  <- 7

# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
                   data = EEG.dm.train, 
                   nrounds = nround,
                   nfold = cv.nfold,
                   verbose = FALSE,
                   prediction = TRUE)

OOF_prediction <- data.frame(cv_model$pred) %>%
  mutate(max_prob = max.col(., ties.method = "last"),
         label = train.label + 1)
head(OOF_prediction)

confusionMatrix(factor(OOF_prediction$max_prob),
                factor(OOF_prediction$label),
                mode = "everything")

```

Hyper-parameter tuning

```{r}
# take parameters from CV  data
bst_model <- xgb.train(params = xgb_params,
                       data = EEG.dm.train,
                       nrounds = 1000, 
                       early_stopping_rounds = 10,
                       watchlist = list(val1 = EEG.dm.train, val2 = EEG.dm.test))



# Predict hold-out test set
test_pred <- predict(bst_model, newdata = EEG.dm.test)
test_prediction <- matrix(test_pred, nrow = numberOfClasses,
                          ncol=length(test_pred)/numberOfClasses) %>%
  t() %>%
  data.frame() %>%
  mutate(label = test.label + 1,
         max_prob = max.col(., "last"))
# confusion matrix of test set
confusionMatrix(factor(test_prediction$max_prob),
                factor(test_prediction$label),
                mode = "everything")

# get the feature real names
names <-  colnames(EEG.db.train[,-1])
# compute feature importance matrix
importance_matrix = xgb.importance(feature_names = names, model = bst_model)
xgb.ggplot.importance(importance_matrix)

```

## Bimodel model:
### Catigories of Effort:

Normal = 6 -18
Above Normal = 18 - 24

```{r}
EEG.db.train = as.data.table(EEG.db[['train']])
EEG.db.test = as.data.table(EEG.db[['test']])

# grouping variables need to be 'one hot' 
# find factor variables
colnames(Filter(is.factor, EEG.db.train))
Group.Matrix.train = model.matrix(~Group-1, EEG.db.train)
Group.Matrix.test = model.matrix(~Group-1, EEG.db.test)

# remove group variable
EEG.db.train[,Group :=NULL]
EEG.db.test[,Group := NULL]

# merge 
EEG.db.train = cbind(EEG.db.train, Group.Matrix.train)
EEG.db.test = cbind(EEG.db.test, Group.Matrix.test)

# Make grouping variables for categories of effort 
# Effort categories: 0= below normal, 1 = normal, 2 = above normal
EEG.db.train$Cat_Effort = as.numeric(ifelse(EEG.db.train$Mean.LE.SNR <= 18, '0', 
                     ifelse(EEG.db.train$Mean.LE.SNR < 24, '1','1')))

EEG.db.test$Cat_Effort = as.numeric(ifelse(EEG.db.test$Mean.LE.SNR <= 18, '0', 
                     ifelse(EEG.db.test$Mean.LE.SNR < 24, '1','1')))

# # Make grouping matrix
# train.label = model.matrix(~Cat_Effort-1, EEG.db.train)
# test.label = model.matrix(~Cat_Effort-1, EEG.db.test)

# Make label matrix
train.label = as.matrix(EEG.db.train$Cat_Effort)
test.label = as.matrix(EEG.db.test$Cat_Effort)

# # remove group variable
# EEG.db.train[,Cat_Effort :=NULL]
# EEG.db.test[,Cat_Effort := NULL]
# 
# # merge 
# EEG.db.train = cbind(EEG.db.train, Cat_Effort.Matrix.train)
# EEG.db.test = cbind(EEG.db.test, Cat_Effort.Matrix.test)

# get label, out outcome variable and make it separate
# train.label = EEG.db.train$Cat_Effort
EEG.db.train[, Cat_Effort :=NULL]
# test.label = EEG.db.test$Cat_Effort
EEG.db.test[, Cat_Effort :=NULL]

# Remove continuous outcome var
EEG.db.train[, Mean.LE.SNR :=NULL]
EEG.db.test[, Mean.LE.SNR :=NULL]
```

## Catagorical using only clinic measures

```{r}
#limit to only clinic vars
EEG.db.train.clinic = EEG.db.train[, c("Sub", "Age","Education","MoCA","L250","L500","L1000","L2000","L3000", "L4000","L6000",
                 "L8000","R250","R500","R1000","R2000","R3000","R4000", "R6000","R8000","PTA","HF_PTA" )]
EEG.db.test.clinic = EEG.db.test[, c("Sub", "Age","Education","MoCA","L250","L500","L1000","L2000","L3000", "L4000","L6000",
                 "L8000","R250","R500","R1000","R2000","R3000","R4000", "R6000","R8000","PTA","HF_PTA" )]

# change into Dmatrix, remove all non-numberic columns (Sub)
EEG.dm.train = xgb.DMatrix(data = as.matrix(EEG.db.train.clinic[,-1]), label = train.label)
EEG.dm.test = xgb.DMatrix(data = as.matrix(EEG.db.test.clinic[,-1]), label = test.label)
```


```{r}
numberOfClasses <- length(unique(test.label))
xgb_params <- list("objective" = "binary:logistic", 
                   "eval_metric" = "logloss")
nround    <- 175 # number of rounds

# CV folds, how many folds
 # length(train.label)/
cv.nfold  <- 7

# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
                   data = EEG.dm.train, 
                   nrounds = nround,
                   nfold = cv.nfold,
                   verbose = FALSE,
                   prediction = TRUE)

OOF_prediction <- data.frame(cv_model$pred) %>%
  mutate(max_prob = max.col(., ties.method = "last"),
         label = train.label + 1)
head(OOF_prediction)

confusionMatrix(factor(OOF_prediction$max_prob),
                factor(OOF_prediction$label),
                mode = "everything")

```

Hyper-parameter tuning

```{r}
# take parameters from CV  data
bst_model <- xgb.train(params = xgb_params,
                       data = EEG.dm.train,
                       nrounds = 1000, 
                       early_stopping_rounds = 10,
                       watchlist = list(val1 = EEG.dm.train, val2 = EEG.dm.test))



# Predict hold-out test set
test_pred <- predict(bst_model, newdata = EEG.dm.test)

prediction = as.numeric(test_pred > 0.5)

# confusion matrix of test set
confusionMatrix(factor(prediction),
                factor(test.label),
                mode = "everything")

# get the feature real names
names <-  colnames(EEG.db.train[,-1])
# compute feature importance matrix
importance_matrix = xgb.importance(feature_names = names, model = bst_model)
xgb.ggplot.importance(importance_matrix)

```

## Compare to logistic regression

```{r}
EEG.db.train.clinic$label = train.label
EEG.db.test.clinic$label = test.label

EEG.lm.model = lm(label ~ ., data=EEG.db.train.clinic[,-1], family = 'binomial')
pred = predict(EEG.lm.model, EEG.db.test.clinic, type = 'response')

prediction = as.numeric(pred > 0.5)

# confusion matrix of test set
confusionMatrix(factor(prediction),
                factor(test.label),
                mode = "everything")


```

